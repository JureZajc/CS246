# -*- coding: utf-8 -*-
"""CS246-2dPy2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13-RbC2SZfyQ4_Q0GUdVoZ15UrUt588qZ

Must needed setup before every assignment
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

"""Imports"""

import itertools
from pyspark import SparkContext, SparkConf
from pyspark.sql import *
from pyspark.sql.functions import *

"""Setup Spark"""

# create the Spark Session
spark = SparkSession.builder.getOrCreate()

# create the Spark Context
sc = spark.sparkContext

Code Variables

s        = 100 # set support threshold
topN     = 20  # set top items to show
fileName = 'browsing.txt'

"""Code Functions"""

def single_and_pairs(line):
  items  = line.split()
  single = [ ( ( item, 'Total'), 1 )  for item in items ]
  pairs  = [ ( pair, 1 ) for pair in itertools.permutations(items, 2)]
  return single + pairs

def sort_filter(counts): #and put the total in the end
  total, results = 0, sorted(list(counts), key=lambda (key, count): -count)
  for key, count in results:
    if key == 'Total':
      total = count
  if total > s:
    results.remove(('Total', total))
    results.append(('Total', total))
    return results #so Total will be always at the end.

def calcCS(pair):
  k1, counts, total = pair[0], pair[1], 0
  if counts != None:
    total = float( counts.pop(-1)[1] )
    return [ ((k1, k2), count/total) for k2, count in counts ]

"""Output"""

confidenceRDD = (sc.textFile(fileName, 12 ) #partition goes here
                  .flatMap( single_and_pairs )
                  .reduceByKey(lambda a, b: a + b)
                  .map(lambda ((key, other), count):(key, (other, count)))
                  .groupByKey()
                  .mapValues( sort_filter )
                  .filter(lambda (key, values): values != None)
                  .flatMap( calcCS )
                  )

print "Top " + str(topN)
#print confidenceRDD.collect()
print confidenceRDD.takeOrdered(topN, lambda (itemSet, score): -score)